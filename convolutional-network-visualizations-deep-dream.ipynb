{"cells":[{"metadata":{"_uuid":"a3e6a588780b2161ddd38479bac77164d99cd00b","_cell_guid":"5d3e34be-95f4-4e5f-b5b3-80ce75d8a428"},"cell_type":"markdown","source":"![](https://images.fastcompany.net/image/upload/w_937,ar_16:9,c_fill,g_auto,f_auto,q_auto,fl_lossy/fc/3048941-poster-p-1-why-google-deep-dreams-of-dogs.jpg)\n# Convolutional Neural Networks Visualization in Pytorch\n\nIn this kernel, we'll look into a convolutional network, to try and understand how they work by generating images that maximize the activation of the filters in the convolutional layers.\nTo generate these images, we apply gradient ascent to the inputs (which will be images with random noise)"},{"metadata":{"_uuid":"a8d45e892df93c2ba4f1a11a8cc4fa7a63c7ac5e","_cell_guid":"cf1d4afa-28c8-4e95-be07-5d13211192e9"},"cell_type":"markdown","source":"## Packages and utils functions\n\nSome util functions to load and visualize images"},{"metadata":{"_uuid":"04ea043d76780d5dd39f8d6673b9d6b850b8bdff","_cell_guid":"d8437be7-1af2-4056-9c57-1c751ec71b79","colab_type":"code","id":"6uznQ7thrODO","colab":{"autoexec":{"startup":false,"wait_interval":0}},"trusted":false,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom torch.optim import SGD\nfrom torchvision import models, transforms\nimport PIL\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\nfrom IPython.display import HTML\n\nimport scipy.ndimage as ndimage\n\n%matplotlib inline\n\nimport scipy.ndimage as nd\nimport PIL.Image\nfrom IPython.display import clear_output, Image, display\nfrom io import BytesIO\n\n\ndef showarray(a, fmt='jpeg'):\n    a = np.uint8(np.clip(a, 0, 255))\n    f = BytesIO()\n    PIL.Image.fromarray(a).save(f, fmt)\n    display(Image(data=f.getvalue()))\n    \ndef showtensor(a):\n    mean = np.array([0.485, 0.456, 0.406]).reshape([1, 1, 3])\n    std = np.array([0.229, 0.224, 0.225]).reshape([1, 1, 3])\n    inp = a[0, :, :, :]\n    inp = inp.transpose(1, 2, 0)\n    inp = std * inp + mean\n    inp *= 255\n    showarray(inp)\n    clear_output(wait=True)\n\ndef plot_images(im, titles=None):\n    plt.figure(figsize=(30, 20))\n    \n    for i in range(len(im)):\n        plt.subplot(10 / 5 + 1, 5, i + 1)\n        plt.axis('off')\n        if titles is not None:\n            plt.title(titles[i])\n        plt.imshow(im[i])\n        \n    plt.pause(0.001)\n    \nnormalise = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nnormalise_resize = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ndef init_image(size=(400, 400, 3)):\n    img = PIL.Image.fromarray(np.uint8(np.full(size, 150)))\n    img = PIL.Image.fromarray(np.uint8(np.random.uniform(150, 180, size)))\n    img_tensor = normalise(img).unsqueeze(0)\n    img_np = img_tensor.numpy()\n    return img, img_tensor, img_np\n\ndef load_image(path, resize=False, size=None):\n    img = PIL.Image.open(path)\n    \n    if size is not None:\n        img.thumbnail(size, PIL.Image.ANTIALIAS)\n        \n    if resize:\n        img_tensor = normalise_resize(img).unsqueeze(0)\n    else:\n        img_tensor = normalise(img).unsqueeze(0)\n    img_np = img_tensor.numpy()\n    return img, img_tensor, img_np\n\ndef tensor_to_img(t):\n    a = t.numpy()\n    mean = np.array([0.485, 0.456, 0.406]).reshape([1, 1, 3])\n    std = np.array([0.229, 0.224, 0.225]).reshape([1, 1, 3])\n    inp = a[0, :, :, :]\n    inp = inp.transpose(1, 2, 0)\n    inp = std * inp + mean\n    inp *= 255\n    inp = np.uint8(np.clip(inp, 0, 255))\n    return PIL.Image.fromarray(inp)\n\ndef image_to_variable(image, requires_grad=False, cuda=False):\n    if cuda:\n        image = Variable(image.cuda(), requires_grad=requires_grad)\n    else:\n        image = Variable(image, requires_grad=requires_grad)\n    return image","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec5ad7387662324efc96241675dced3c1e1efd3d","_cell_guid":"81f70ff4-8df4-49b3-affa-35d7258b9238"},"cell_type":"markdown","source":"## Model Creation\n\nHere we load a pretrained VGG-16 model"},{"metadata":{"_uuid":"89b7471b46c7ca2c71ca61ef7d4951eed0f7311f","colab":{"autoexec":{"startup":false,"wait_interval":0}},"_cell_guid":"271c55bc-ca04-4b81-90ef-8d34fc7fc4d9","collapsed":true,"colab_type":"code","id":"RhQv-47lrODU","trusted":false},"cell_type":"code","source":"model = models.vgg16()\nmodel.load_state_dict(torch.load(\"../input/vgg16/vgg16.pth\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f62aa020ad0899460defa9e49be8d9f89c531a67","_cell_guid":"c734253c-7d4d-4afb-bfef-685cac98288e","executionInfo":{"elapsed":2770,"status":"ok","timestamp":1526052553869,"user":{"photoUrl":"//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg","userId":"107843268563316278814","displayName":"Carlo Alberto"},"user_tz":-120},"colab_type":"code","id":"4aRAE7SqzPCK","outputId":"f10a5dda-457b-4989-a801-d4ba16a3355c","colab":{"autoexec":{"startup":false,"wait_interval":0},"height":765,"base_uri":"https://localhost:8080/"},"trusted":false,"collapsed":true},"cell_type":"code","source":"use_gpu = False\nif torch.cuda.is_available():\n    use_gpu = True\n\nprint(model)\n\nfor param in model.parameters():\n    param.requires_grad = False\n\nif use_gpu:\n    print(\"Using CUDA\")\n    model.cuda()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f972135ef1068a6ef3b644d15459159ee0dbacb7","_cell_guid":"cea19113-e7c9-47a7-ab1d-b9178ef990b1","colab_type":"text","id":"7Vl6h9rjzgJ-"},"cell_type":"markdown","source":"## Octaver function\n\nThe octaver function is used to produce better output images. This procedure is taken from the [Deep Dream code](https://github.com/google/deepdream/blob/master/dream.ipynb). \nWe will use it not only for the deep dream algorithm, but also for visualizing the filters.\nThe gradient ascent algorithm is run on multiple downscaled versions of the image, and the results are upscaled and merged together to get the final image.\n\n![](https://raw.githubusercontent.com/Hvass-Labs/TensorFlow-Tutorials/master/images/14_deepdream_recursive_flowchart.png)\n*Figure 1: Flowchart of the octaver function + deep dream, taken from https://github.com/Hvass-Labs/TensorFlow-Tutorials*"},{"metadata":{"_uuid":"1ef4e55fd8179faa8f2ee35016cae9c8b1e1e113","colab":{"autoexec":{"startup":false,"wait_interval":0}},"_cell_guid":"fe851140-9d40-47ea-be78-517bc4a2238a","collapsed":true,"colab_type":"code","id":"dxd0ATHnzgJ_","trusted":false},"cell_type":"code","source":"def octaver_fn(model, base_img, step_fn, octave_n=6, octave_scale=1.4, iter_n=10, **step_args):\n    octaves = [base_img]\n    \n    for i in range(octave_n - 1):\n        octaves.append(nd.zoom(octaves[-1], (1, 1, 1.0 / octave_scale, 1.0 / octave_scale), order=1))\n\n    detail = np.zeros_like(octaves[-1])\n    for octave, octave_base in enumerate(octaves[::-1]):\n        h, w = octave_base.shape[-2:]\n        \n        if octave > 0:\n            h1, w1 = detail.shape[-2:]\n            detail = nd.zoom(detail, (1, 1, 1.0 * h / h1, 1.0 * w / w1), order=1)\n        \n        src = octave_base + detail\n        \n        for i in range(iter_n):\n            src = step_fn(model, src, **step_args)\n\n        detail = src.numpy() - octave_base\n\n    return src","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f941397cbeaf6a6da3b7b2f7c52817909cb87f7","_cell_guid":"85f7db57-66a7-46d9-a30c-cf13d44d55a0","colab_type":"text","id":"i1gG6SapzgKD"},"cell_type":"markdown","source":"## Filter Visualization\n\nThis function produces an image that maximizes the activation of the filter at *filter_index* in the layer *layer_index*. "},{"metadata":{"_uuid":"0e6bb7328fa31ac1a174fd8101411d7c90fc7ba6","colab":{"autoexec":{"startup":false,"wait_interval":0}},"_cell_guid":"27c0ae4b-d6e5-430c-86dc-930110822bbd","collapsed":true,"colab_type":"code","id":"Sx1jZfDJzgKE","trusted":false},"cell_type":"code","source":"def filter_step(model, img, layer_index, filter_index, step_size=5, display=True, use_L2=False):\n    global use_gpu\n    \n    mean = np.array([0.485, 0.456, 0.406]).reshape([3, 1, 1])\n    std = np.array([0.229, 0.224, 0.225]).reshape([3, 1, 1])\n    \n    model.zero_grad()\n    \n    img_var = image_to_variable(torch.Tensor(img), requires_grad=True, cuda=use_gpu)\n    optimizer = SGD([img_var], lr=step_size, weight_decay=1e-4)\n    \n    x = img_var\n    for index, layer in enumerate(model.features):\n        x = layer(x)\n        if index == layer_index:\n            break\n\n    output = x[0, filter_index]\n    loss = output.norm() #torch.mean(output)\n    loss.backward()\n    \n    if use_L2:\n        #L2 normalization on gradients\n        mean_square = torch.Tensor([torch.mean(img_var.grad.data ** 2) + 1e-5])\n        if use_gpu:\n            mean_square = mean_square.cuda()\n        img_var.grad.data /= torch.sqrt(mean_square)\n        img_var.data.add_(img_var.grad.data * step_size)\n    else:\n        optimizer.step()\n    \n    result = img_var.data.cpu().numpy()\n    result[0, :, :, :] = np.clip(result[0, :, :, :], -mean / std, (1 - mean) / std)\n    \n    if display:\n        showtensor(result)\n    \n    return torch.Tensor(result)\n\ndef visualize_filter(model, base_img, layer_index, filter_index, \n                     octave_n=6, octave_scale=1.4, iter_n=10, \n                     step_size=5, display=True, use_L2=False):\n    \n    return octaver_fn(\n                model, base_img, step_fn=filter_step, \n                octave_n=octave_n, octave_scale=octave_scale, \n                iter_n=iter_n, layer_index=layer_index, \n                filter_index=filter_index, step_size=step_size, \n                display=display, use_L2=use_L2\n            )\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"571146fbd4f5a44dd81f1fb5ea97918800966e49","_cell_guid":"88f89e41-5fcc-437a-90b5-8e3c472b1740"},"cell_type":"markdown","source":"Next, we define a helper function to visualize a number of filter for a given layer"},{"metadata":{"_uuid":"aa0a667e95b0a82666aaaab97064074d05d66d73","colab":{"autoexec":{"startup":false,"wait_interval":0}},"_cell_guid":"ddd34a1c-1e38-4f62-90e2-1e327ba20e5a","collapsed":true,"colab_type":"code","id":"teKi_nzKzgKH","trusted":false},"cell_type":"code","source":"def show_layer(layer_num, filter_start=10, filter_end=20, step_size=7, use_L2=False):\n    filters = []\n    titles = []\n    \n    _, _, img_np = init_image(size=(600, 600, 3))\n    for i in range(filter_start, filter_end):\n        title = \"Layer {} Filter {}\".format(layer_num , i)\n        print(title)\n        filter = visualize_filter(model, img_np, layer_num, filter_index=i, octave_n=2, iter_n=20, step_size=step_size, display=True, use_L2=use_L2)\n        filter_img = tensor_to_img(filter)\n        filter_img.save(title + \".jpg\")\n        filters.append(tensor_to_img(filter))\n        titles.append(title)\n        \n    \n    plot_images(filters, titles)\n    return filters, titles","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"282422e5ccc9f792c20b82208f26cbcee89ad834","_cell_guid":"3106447b-883d-4664-a765-ab195936d588","trusted":false,"collapsed":true},"cell_type":"code","source":"images, titles = show_layer(1, use_L2=True, step_size=0.05)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d000e5c39593adfe4be64e8751d8ccbb14e5c4b","colab":{"autoexec":{"startup":false,"wait_interval":0},"height":735,"base_uri":"https://localhost:8080/"},"_cell_guid":"8eaa0496-dbf4-4f88-bcf4-697afd22b4c5","collapsed":true,"executionInfo":{"elapsed":379,"status":"ok","timestamp":1526052782636,"user":{"photoUrl":"//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg","userId":"107843268563316278814","displayName":"Carlo Alberto"},"user_tz":-120},"colab_type":"code","outputId":"9c9421a6-230c-4af5-df1c-2a76683fa6c4","id":"hMjh0FtozgKL","trusted":false},"cell_type":"code","source":"images, titles = show_layer(10, use_L2=True, step_size=0.05)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18d517848c01911e86ef3405214e58a300134c47","colab":{"autoexec":{"startup":false,"wait_interval":0},"height":735,"base_uri":"https://localhost:8080/"},"_cell_guid":"6c76af4b-36f4-40d8-996e-700fda449ea3","collapsed":true,"executionInfo":{"elapsed":339,"status":"ok","timestamp":1526052896119,"user":{"photoUrl":"//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg","userId":"107843268563316278814","displayName":"Carlo Alberto"},"user_tz":-120},"colab_type":"code","outputId":"1b924cff-95b3-4238-9eb1-610c374cb16a","id":"zNIt9cmlzgKN","trusted":false},"cell_type":"code","source":"images, titles = show_layer(14, use_L2=True, step_size=0.05)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef64ac690372a172c5fc0a2b867dd0467f922486","colab":{"autoexec":{"startup":false,"wait_interval":0},"height":735,"base_uri":"https://localhost:8080/"},"_cell_guid":"fe2a6488-7334-4047-a75c-6e524b2df038","collapsed":true,"executionInfo":{"elapsed":381,"status":"ok","timestamp":1526053022082,"user":{"photoUrl":"//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg","userId":"107843268563316278814","displayName":"Carlo Alberto"},"user_tz":-120},"colab_type":"code","outputId":"f8f4cf34-78cb-4043-ef0d-0b170636daf7","id":"jjl51LgtzgKT","trusted":false},"cell_type":"code","source":"images, titles = show_layer(17, use_L2=True, step_size=0.05)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8bca2cebd3d292f660e0783d3adf70a8ab789c98","colab":{"autoexec":{"startup":false,"wait_interval":0},"height":735,"base_uri":"https://localhost:8080/"},"_cell_guid":"ce718c07-d2e5-4e56-8320-616a9b2aa121","executionInfo":{"elapsed":380,"status":"ok","timestamp":1526053155769,"user":{"photoUrl":"//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg","userId":"107843268563316278814","displayName":"Carlo Alberto"},"user_tz":-120},"collapsed":true,"colab_type":"code","id":"6tGMhYuTzgKX","outputId":"6091257d-e7c9-423e-eae3-7d59bf67e4b1","scrolled":true,"trusted":false},"cell_type":"code","source":"images, titles = show_layer(19, use_L2=True, step_size=0.05)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91f0a4d4351b6b0b44921e3ca7b5526024552706","colab":{"autoexec":{"startup":false,"wait_interval":0},"height":735,"base_uri":"https://localhost:8080/"},"_cell_guid":"6692c252-8038-4ba3-94de-f3a05676e962","collapsed":true,"executionInfo":{"elapsed":358,"status":"ok","timestamp":1526053304729,"user":{"photoUrl":"//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg","userId":"107843268563316278814","displayName":"Carlo Alberto"},"user_tz":-120},"colab_type":"code","outputId":"ce73e00e-c2bd-4a85-df06-786b15da1d1e","id":"tX6rofDdzgKc","trusted":false},"cell_type":"code","source":"images, titles = show_layer(21, use_L2=True, step_size=0.05)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a841351ff9958c3d0533334b0a5025940d894a3b","_cell_guid":"5956e8f6-e91e-4595-9e7f-c43fc0941574","colab_type":"text","id":"jqTQYKbNzQgc"},"cell_type":"markdown","source":"## Deep Dream\n\nThe Deep Dream function is similar to the filter visualization, but instead of starting from a random noise image, we start from an actual picture and try to maximize the network output. In this way, we're enhancing the features that the network recognizes in the image. Different layers yields different results; lower ones tend to procude geometric patterns and textures, while higher ones produce more abstract shapes that resemble what the network has seen during its training process.\n\nThe actual code is ported to pytorch from the [Google Deep Dream repository](https://github.com/google/deepdream) which runs under cafe."},{"metadata":{"_uuid":"d1eaf9df9e44130031b6ce04eb495ac8275e1818","colab":{"autoexec":{"startup":false,"wait_interval":0}},"_cell_guid":"c99d972d-b603-48f8-b3fb-500ba56cdba9","collapsed":true,"colab_type":"code","id":"GN5b7hTJzQgd","trusted":false},"cell_type":"code","source":"def objective(dst, guide_features):\n    if guide_features is None:\n        return dst.data\n    else:\n        x = dst.data[0].cpu().numpy()\n        y = guide_features.data[0].cpu().numpy()\n        ch, w, h = x.shape\n        x = x.reshape(ch, -1)\n        y = y.reshape(ch, -1)\n        A = x.T.dot(y)\n        diff = y[:, A.argmax(1)]\n        diff = torch.Tensor(np.array([diff.reshape(ch, w, h)])).cuda()\n        return diff\n\ndef make_step(model, img, objective=objective, control=None, step_size=1.5, end=28, jitter=32):\n    global use_gpu\n    \n    mean = np.array([0.485, 0.456, 0.406]).reshape([3, 1, 1])\n    std = np.array([0.229, 0.224, 0.225]).reshape([3, 1, 1])\n    \n    ox, oy = np.random.randint(-jitter, jitter+1, 2)\n    \n    img = np.roll(np.roll(img, ox, -1), oy, -2)\n    tensor = torch.Tensor(img) \n    \n    img_var = image_to_variable(tensor, requires_grad=True, cuda=use_gpu)\n    model.zero_grad()\n      \n    x = img_var\n    for index, layer in enumerate(model.features.children()):\n        x = layer(x)\n        if index == end:\n            break\n    \n    delta = objective(x, control)\n    x.backward(delta)\n    \n    #L2 Regularization on gradients\n    mean_square = torch.Tensor([torch.mean(img_var.grad.data ** 2)])\n    if use_gpu:\n        mean_square = mean_square.cuda()\n    img_var.grad.data /= torch.sqrt(mean_square)\n    img_var.data.add_(img_var.grad.data * step_size)\n    \n    result = img_var.data.cpu().numpy()\n    result = np.roll(np.roll(result, -ox, -1), -oy, -2)\n    result[0, :, :, :] = np.clip(result[0, :, :, :], -mean / std, (1 - mean) / std)\n    showtensor(result)\n    \n    return torch.Tensor(result)\n                                                             \ndef deepdream(model, base_img, octave_n=6, octave_scale=1.4, \n              iter_n=10, end=28, control=None, objective=objective, \n              step_size=1.5, jitter=32):\n    \n    return octaver_fn(\n              model, base_img, step_fn=make_step, \n              octave_n=octave_n, octave_scale=octave_scale, \n              iter_n=iter_n, end=end, control=control,\n              objective=objective, step_size=step_size, jitter=jitter\n           )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"863fb94188c95aef2aa6f3bbb786f697eab8ed43","colab":{"autoexec":{"startup":false,"wait_interval":0},"height":747,"base_uri":"https://localhost:8080/"},"_cell_guid":"149ca97f-00b7-43c3-a8e0-871f8efe77d3","collapsed":true,"executionInfo":{"elapsed":4410,"status":"ok","timestamp":1526053309796,"user":{"photoUrl":"//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg","userId":"107843268563316278814","displayName":"Carlo Alberto"},"user_tz":-120},"colab_type":"code","outputId":"bd9f48b8-c66a-43fc-a987-f7d31920af0b","id":"XZB9hS2zyVLt","trusted":false},"cell_type":"code","source":"input_img, input_tensor, input_np = load_image('../input/sampleimages/data/data/market1.jpg', size=[1024, 1024])\nprint(input_img.size)\ninput_img","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4247b2ceb5987ee6532a8681b38a28708125102","colab":{"autoexec":{"startup":false,"wait_interval":0},"height":730,"base_uri":"https://localhost:8080/"},"_cell_guid":"d045dbee-8cba-47f6-9ff6-c8e40a3d3667","collapsed":true,"executionInfo":{"elapsed":11,"status":"ok","timestamp":1526053406741,"user":{"photoUrl":"//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg","userId":"107843268563316278814","displayName":"Carlo Alberto"},"user_tz":-120},"colab_type":"code","outputId":"4b8a8981-85cd-4632-9224-6456ea2c2854","id":"pwPcM7iIyrbV","trusted":false},"cell_type":"code","source":"dream = deepdream(model, input_np, end=14, step_size=0.06, octave_n=6)\ndream = tensor_to_img(dream)\ndream.save('dream00.jpg')\ndream","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23d690a7029dbc5721257dd36f9f17a45f9e208b","colab":{"autoexec":{"startup":false,"wait_interval":0},"height":730,"base_uri":"https://localhost:8080/"},"_cell_guid":"8b6a183f-1170-4d05-b563-0c3a2d1d7d5a","collapsed":true,"executionInfo":{"elapsed":12,"status":"ok","timestamp":1526053449648,"user":{"photoUrl":"//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg","userId":"107843268563316278814","displayName":"Carlo Alberto"},"user_tz":-120},"colab_type":"code","outputId":"7db62efb-942d-4171-8553-2113c52b08e8","id":"dCMmcfrv3zNz","trusted":false},"cell_type":"code","source":"dream = deepdream(model, input_np, end=20, step_size=0.06, octave_n=6)\ndream = tensor_to_img(dream)\ndream.save('dream01.jpg')\ndream","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48497501c7fca8439b4808a1d22057b1688ab4fb","colab":{"autoexec":{"startup":false,"wait_interval":0},"height":730,"base_uri":"https://localhost:8080/"},"_cell_guid":"36556f61-5853-4a27-b42b-7921d822d79e","collapsed":true,"executionInfo":{"elapsed":13,"status":"ok","timestamp":1526053724311,"user":{"photoUrl":"//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg","userId":"107843268563316278814","displayName":"Carlo Alberto"},"user_tz":-120},"colab_type":"code","outputId":"3186785b-d42c-453e-874a-83b654cdbc22","id":"mIqd4Y7s374S","trusted":false},"cell_type":"code","source":"dream = deepdream(model, input_np, end=28, step_size=0.06, octave_n=6)\ndream = tensor_to_img(dream)\ndream.save('dream03.jpg')\ndream","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0dd0c071e08f2d0378b2ae6e85763948fe27494a","_cell_guid":"ae39a43c-ff5c-4b17-b29e-3be651da2204"},"cell_type":"markdown","source":"## Controlling the dream\n\nWe can control the dream by trying to alter the image in order to maximize the filters that are activated by another image (which we'll call guide)"},{"metadata":{"_uuid":"588da3147d76131c49213113be27ee6b8fc44b3c","colab":{"autoexec":{"startup":false,"wait_interval":0},"height":260,"base_uri":"https://localhost:8080/"},"_cell_guid":"7fc0ddc6-9caa-40da-a6ac-382046d27ebd","collapsed":true,"executionInfo":{"elapsed":1017,"status":"ok","timestamp":1526054100646,"user":{"photoUrl":"//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg","userId":"107843268563316278814","displayName":"Carlo Alberto"},"user_tz":-120},"colab_type":"code","outputId":"7a169f5b-1d77-4de3-b837-c8e4e29abfa0","id":"S97NHU7sY8Yf","trusted":false},"cell_type":"code","source":"guide_img, guide_img_tensor, guide_img_np = load_image('../input/sampleimages/data/data/kitten2.jpg', resize=True)\nplt.imshow(guide_img)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2cabeef0eb4ef98b9fa1a3c982833065429bb2d","colab":{"autoexec":{"startup":false,"wait_interval":0},"height":730,"base_uri":"https://localhost:8080/"},"_cell_guid":"2fe6d591-8926-4f48-b4a9-d2ead3081649","collapsed":true,"executionInfo":{"elapsed":17,"status":"ok","timestamp":1526054232473,"user":{"photoUrl":"//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg","userId":"107843268563316278814","displayName":"Carlo Alberto"},"user_tz":-120},"colab_type":"code","outputId":"4ebbbf4a-4bef-45a5-ed55-14aa40435172","id":"tSJ3RNxaZlkH","trusted":false},"cell_type":"code","source":"end = 26\n\nguide_features = image_to_variable(guide_img_tensor, cuda=use_gpu)\n\nfor index, layer in enumerate(model.features.children()):\n    guide_features = layer(guide_features)\n    if index == end:\n        break\n    \ndream = deepdream(model, input_np, end=end, step_size=0.06, octave_n=4, control=guide_features)\ndream = tensor_to_img(dream)\ndream.save('dream04.jpg')\ndream","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a29dff0df52c5e6b8001310161bbb46e673dd5a9","colab":{"autoexec":{"startup":false,"wait_interval":0}},"_cell_guid":"af1a3688-453f-4a2c-89d4-6be1fae9ae5e","collapsed":true,"colab_type":"code","id":"6nfWCLRyzgK4","trusted":false},"cell_type":"code","source":"input_img, input_tensor, input_np = load_image('../input/sampleimages/data/data/face1.jpg', size=[1024, 1024])\nprint(input_img.size)\ninput_img","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1d225814fb705ea40e1d8f34abc764f1d288224","_cell_guid":"64eaa80e-619e-4855-ae93-820afe67fabc","collapsed":true,"trusted":false},"cell_type":"code","source":"dream = deepdream(model, input_np, end=26, step_size=0.06, octave_n=6, control=guide_features)\ndream = tensor_to_img(dream)\ndream.save('dream05.jpg')\ndream","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88c74a6ca826d6df669073d6158f151b0d1c87ab","_cell_guid":"8e45a204-9601-46d9-ad87-d670d86ed9a7"},"cell_type":"markdown","source":"## References\n\n- How Convolutional Neural Networks see the world - Keras Blog https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html\n- Google Deep Dream code https://github.com/google/deepdream\n- Inceptionism: Going Deeper into Neural Networks https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html\n- PyTorch DeepDream https://github.com/L1aoXingyu/Deep-Dream\n- DL06: DeepDream (with code) https://hackernoon.com/dl06-deepdream-with-code-5f735052e21f\n- https://www.youtube.com/watch?v=ws-ZbiFV1Ms\n- TensorFlow-Tutorials https://github.com/Hvass-Labs/TensorFlow-Tutorials"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"default_view":{},"provenance":[],"name":"CNN Visualization.ipynb","collapsed_sections":[],"version":"0.3.2","views":{}}},"nbformat":4,"nbformat_minor":1}